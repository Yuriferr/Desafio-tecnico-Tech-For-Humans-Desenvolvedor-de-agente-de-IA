# üè¶ Banco √Ågil - Assistente IA (Tech For Humans)

<div align="center">
  <img src="images/capa.png" alt="Capa do Projeto - Banco √Ågil" width="100%">
</div>

Ol√°! Bem-vindo ao reposit√≥rio do **Banco √Ågil**. Criei este projeto como solu√ß√£o para o **Desafio T√©cnico para Desenvolvedor de Agente de IA da Tech For Humans**. 

Meu objetivo principal aqui foi construir um sistema de chat inteligente avan√ßado, onde o usu√°rio sinta que est√° conversando com um √∫nico c√©rebro super capaz (o que chamamos de *Single-Agent Illusion*), mas que, por debaixo dos panos, rodam diversos micro-agentes especializados trabalhando em conjunto sob a orquestra√ß√£o de um modelo de linguagem local.

---

## üõ† A Minha Stack (Escolhas T√©cnicas e Justificativas)

Para entregar uma solu√ß√£o moderna, r√°pida e aderente ao escopo do desafio, optei pela seguinte stack de tecnologias:

- **Ollama + Llama 3.2**: Eu quis provar que o projeto poderia ter autonomia completa rodando `Local-First`, sem depender minimamente de faturamentos ou chaves da OpenAI. O Llama 3.2 √© um modelo open-source espetacular, muito perform√°tico at√© em firmwares modestos, e com uma capacidade de racioc√≠nio de alto n√≠vel, ideal para ser o cerne da l√≥gica dos nossos agentes.
- **LangChain**: O LangChain facilitou demais a integra√ß√£o com o Ollama, mas o principal motivo da minha escolha ocorreu devido aos seus "Output Parsers" e encadeamentos (*Chains*). Com ele, eu n√£o recebo apenas um bloco de texto bagun√ßado da IA; consegui for√ßar o rob√¥ a entregar dados estruturados (JSON Strict), o que me permitiu extrair as vari√°veis exatas na hora da entrevista financeira do usu√°rio.
- **Python + FastAPI**: O *core* da aplica√ß√£o. Escolhi o FastAPI por ser enxuto, extremamente r√°pido e, o mais importante, ass√≠ncrono por padr√£o (`async def`). Ao lidar com IAs e chamadas HTTP externas (como na cota√ß√£o de moedas), o assincronismo √© vital para manter o sistema responsivo. O sistema de roteadores pr√°tico (`APIRouter`) tamb√©m ajudou imensamente a segmentar meus agentes.
- **Vanilla JS + HTML + CSS**: No frontend, optei por trabalhar na "unha", sem usar frameworks como React ou Vue. Por se tratar de um desafio de Agente focado em backend, eu quis garantir que quem fosse avaliar pudesse apenas "dar dois cliques" no index.html e rodar tudo, sem se preocupar em baixar pacotes infinitos (`node_modules`) ou configurar build-tools. As chamadas ass√≠ncronas no frontend garantem muita agilidade, complementando perfeitamente a API.
- **Arquivos CSV**: Como banco de dados das valida√ß√µes de regras e scores, utilizei abordagens diretas lendo arquivos `.csv`. Foi uma escolha puramente focada na efici√™ncia de testes, dispensando que voc√™ precise levantar um container do PostgreSQL ou Mongo para brincar com o bot.

---

## üìñ Vis√£o Geral do Projeto

A ideia do sistema √© aposentar os jur√°ssicos fluxos num√©ricos de URA ("Digite 1 para X"). Como resultado, temos aqui a infer√™ncia de rotas alimentada puramente por contexto (*NLP*). 

O cliente diz em linguagem humana: *"Eita, meu limite t√° baixo, d√° um jeito nisso?"*. Nesse momento, a nossa intelig√™ncia analisa a sem√¢ntica da frase e despacha a requisi√ß√£o pro Agente correto. Caso o aumento n√£o seja vi√°vel, ao inv√©s do processo morrer, o sistema engatilha dinamicamente uma oportunidade: prop√µe ao usu√°rio uma r√°pida Entrevista de Rec√°lculo de Score para avaliar seus dados de forma humanizada.

---

## üèó Arquitetura do Sistema e Fluxos

Em vez de empilhar mil linhas em um s√≥ arquivo, criei um sistema puramente modular. Nossa Arquitetura possui 2 pilares: Servi√ßos Core Globais e os Agentes Independentes em Roteadores menores (`api/routers/`).

### 1. Sistema Multi-Agente
- üö™ **Agente de Triagem (`triagem.py`)**: O anfitri√£o do banco. √â ele quem faz o "handshake" validando CPF e Data de Nascimento no nosso CSV. Depois de liberar o acesso, ele pergunta a vontade do cliente, interpreta a NLP, transfere o status ativamente para a pr√≥xima etapa em sil√™ncio e desaparece se sentindo bem-sucedido.
- üí≥ **Agente de Cr√©dito (`credito.py`)**: Especialista em regras de neg√≥cio. Ele quem cruza o score atual e verifica a pol√≠tica ("Score 400 permite Limite Y?"). Quando os c√°lculos batem no teto limite de forma negativa, ele atua ativamente engatilhando a nossa sub-rotina do Perito de Entrevistas.
- üìã **Agente de Entrevista (`entrevista.py`)**: O Perito de Risco de Convers√£o. Bate um papo simples para coletar informa√ß√µes base: Renda, Status de Emprego, Dependentes, Despesas e D√≠vidas. Pega todo o "lero-lero" falado pelo usu√°rio, e usa o LangChain para compilar em um JSON, calculando a macrof√≥rmula matem√°tica que injeta via IO no CSV e eleva a chance do cliente. Re-transfere a aprova√ß√£o para a malha do fluxo de cr√©dito logrando √™xito autom√°tico.
- üåç **Agente de C√¢mbio (`cambio.py`)**: Consome a API REST gratuita (AwesomeAPI). Com a intelig√™ncia local, entende desde jarg√µes isolados a perguntas polidas. Passando "O euro eita," ele extrai `EUR` e puxa a cota√ß√£o imediata convertida na nossa moeda `BRL`.

### 2. Backbones de Controle
- **LLM Service (`llm_service.py`)**: Como um "Data-lake Prompt√°rio", esse arquivo controla o encadeamento e inst√¢ncias do Llama e abriga o Try/Catch anti-pane caso o Hardware local desligue ou retorne um Timeout.
- **Memory Service / Sess√£o (`sessao.py`)**: Gerenciador In-Memory. Segrega o ID de chat da aba do front-end com um vetor persistente (`Role/Message`) mantendo forte controle do hist√≥rico. Tamb√©m gerencia a m√°quina de estados como (`AGUARDANDO_CPF`, `AUTENTICADO`).

---

## üöÄ Funcionalidades que Implementei

- **Single-Agent Illusion**: Bot√µes baseados em 'Outros Servi√ßos'. Os roteamentos (`A√ß√£o Transferir`) nem chegam aos olhos do usu√°rio; parece ser um s√≥ rob√¥ com mil habilidades.
- **Cota√ß√£o Din√¢mica Externa**: Consumo via Request lib para pegar cota√ß√£o ativa da internet.
- **Workflow de Segunda Chance**: Processo interligado onde um score recusado √© submetido sob a decis√£o do usu√°rio a uma rec√°lculo por entrevista ativa em tempo de execu√ß√£o, mudando a recusa do limite para aprovado na mesma conversa.
- **Resili√™ncia de Stack**: O uso exaustivo de valida√ß√£o (Try/Catch) que lida ativamente caso o Llama sofra pane ao cuspir um JSON errado ou o banco CSV demore a ser aberto pelo S.O., re-emitindo um feedback educado contornando que a p√°gina do frontend "morra" esperando sinal.
- **Extrativismo de Dados Indiretos (Typos)**: Testado para ignorar erros de gra√∫do como "siiimbora fzer interwieem" e extrair perfeitamente o Boolean de "Aceitar" para prosseguir no fluxo da API.

---

## ‚öî Desafios Enfrentados (e como eu os resolvi)

V√°rias "cascas de bananas" arquiteturais apareceram, confira tr√™s delas que contornei:

1. **A 'Alucina√ß√£o' no Output limitando fluxos na Triagem**:
   - *Desafio*: Quando perguntei sobre taxas do C√¢mbio, a LLM local em alguns casos divagou nas raz√µes com o cl√°ssico texto "N√£o foi pedido Atualiza√ß√£o Cadastral e sim Cota√ß√£o". Como o IF do python lia a palavra "Atualiza√ß√£o Cadastral" antes da palavra "Cota√ß√£o", o rob√¥ enlouquecia jogando o cliente no limite.
   - *Solu√ß√£o*: Refiz a matriz heur√≠stica de varredura no c√≥digo Python da rota. Engessando para parar a verifica√ß√£o mal encontrasse √† primeir√≠ssima ocorr√™ncia das Tags expl√≠citas que caracterizam as Rotas (`break` condicional). Problema resolvido categoricamente!

2. **JSONs quebrando por respostas booleanas at√≠picas**:
   - *Desafio*: Extrair "sim" e "n√£o" para saber se tinha d√≠vidas n√£o funcionava direto, os usu√°rios podiam escrever coisas como "credo nunca". O `OutputParser` puro explodia por invalidez sem√¢ntica.
   - *Solu√ß√£o*: Implementei o `JsonOutputParser` nativo for√ßando as estruturas usando Temperature √† `0.0`. E, paralelamente, embuti Fallbacks Python no Back-End. ("Se tentar retornar Null nos dependentes, usa heur√≠stica do Python e varre se tem a palavra "n√£o" para fixar o dependente a ZERO, salvando o sistema).

3. **Demora aparente no Front-end gerando atrito de UX**:
   - *Desafio*: O carregamento do Llama demorava entre 1 e 2 segundos. O usu√°rio ficava na estaca zero olhando para uma tela sem sinal achando que o Request explodiu.
   - *Solu√ß√£o*: Recurso Vanilla minimalista: Adicionei Inje√ß√µes do Elemento Ass√≠ncrono com evento de DOM para gerar as benditas "Bolinhas quicando de Carregamento" escondendo que a infraestrutura estava num long-polling aguardando o Status 200 da API Fast.  

---

## üïπ Passo a Passo: Como rodar e testar o projeto

√â muito f√°cil fazer a m√°quina funcionar. Voc√™ testar√° todo o complexo orquestrador em apenas 4 passos:

### Pr√©-requisitos B√°sicos:
Eu estruturei o sistema para usar as seguintes premissas (que eu recomendo ter instalado):
- **Python 3.10+** (Para a Engine de Agentes rodarem via Fast)
- **Ollama** (Motor de IA offline para nos dar infer√™ncia segura). Instale acessando: `https://ollama.com`

### Passo 1. C√©rebro Base (LLM Local)
Abra seu terminal e baixe os pesos do nosso agente executando:
```bash
ollama run llama3.2
```
Ele instalar√° na sua m√°quina *(aprox 2GB)*. Assim que ver um prompt `>>>` esperando sua mensagem, pode fechar a janela com `Ctrl+D` ou sair dele. Todo o sistema do modelo j√° ficou habilitado.

### Passo 2. Ligando a Usina da API Python
Aponte o terminal para a nossa pasta baixada e siga para instalar nossos libs e ligar:
```bash
cd api
# Recomendo muito usar uma VENV (python -m venv venv), caso contr√°rio instale livremente:
pip install -r requirements.txt

# Inicie ativando o comando uvicorn com recarga est√°tica exclusando os csv din√¢micos (Evita re-starts durante os testes simulados de salvar vari√°veis no csv):
python main.py
```
A API vai piscar informando que acordou no IP `[http://0.0.0.0:8000]`

### Passo 3. Abrindo a Porta do Banco
Tudo pronto. Como eu montei o front para se desprender de Node e React para seu total conforto, tudo que voc√™ far√° √© abrir o diret√≥rio base da raiz (fora de `/api/`), ir para a pasta `/frontend/`  e **abrir o arquivo `index.html` em seu navegador** duplo-clique b√°sico.

### Passo 4. Guia Simulado de Uso Pr√°tico (O Teste final):
Divirta-se. Use o seguinte cen√°rio "Golden-Path Completo":

1. Simule como "Jo√£o", envie o CPF simulado (`12345678900`) e passe as datas falsas `01/01/1980`.
2. Como um cliente insatisfeito, ignore as op√ß√µes globais bonitas do template que criei, escreva solto: *"Kero aumentar tudo meus cr√©dito"*.
3. O rob√¥ vai reagir te oferecendo o Menu respectivo. Puxando o bot√£o para o alto digite um limite ignorante de `500000` *(Ou quinhentos mil inteiros)*. Observe como as matrizes internas o bloqueiam por ser irreal para o score de Jo√£o mas logo em seguida abrem a Entrevista!
4. Responda positivamente para a entrevista. Aceite, simule, diga que sua ocupa√ß√£o √© desempregado para observar se ele absorve para o JSON e finaliza o rec√°lculo em aprova√ß√£o limpa.
5. Termine escrevendo: *"Legal, quero ver a cota√ß√£o do BTC (Bitcoin)"* e as bolinhas de chat mostrar√£o o motor chamando as rotas da web trazendo os centavos online no fechamento.

### Enjoy The Ride :D
Espero que voc√™ se divirta testando o c√≥digo tanto quanto eu me diverti desenvolvendo. Foi incr√≠vel poder juntar o ecossistema LLM open-source dentro da casca corporativa da Tech for Humans!