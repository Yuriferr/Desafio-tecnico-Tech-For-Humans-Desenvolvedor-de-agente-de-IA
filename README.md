# üè¶ Banco √Ågil - Assistente IA (Tech For Humans)

Bem-vindo ao reposit√≥rio do **Banco √Ågil**, um sistema inteligente e multi-agente focado no atendimento banc√°rio via chat, desenvolvido para atender ao **Desafio T√©cnico para Desenvolvedor de Agente de IA da Tech For Humans**.

Este projeto simula uma assistente virtual conversacional avan√ßada. O usu√°rio interage com um bot que parece unificado ("Single-Agent Illusion"), mas que arquiteturalmente roda atrav√©s de um sistema Multi-Agent (Routing) orquestrado por LLM local.

---

## üìñ Vis√£o Geral do Projeto

A aplica√ß√£o prov√©m uma interface de chat intuitiva e veloz onde o cliente pode se autenticar e realizar consultas banc√°rias cotidianas. Ao inv√©s de fluxos de navega√ß√£o est√°ticos e presos a menus num√©ricos (como URAs telef√¥nicas), o projeto se baseia em **Infer√™ncia de Inten√ß√£o via NLP (Natural Language Processing)**. 

O cliente pode digitar em linguagem natural o que deseja (ex: "Quero ver a cota√ß√£o do d√≥lar" ou "Meu limite t√° baixo, posso aumentar?") e o c√©rebro principal roteia e processa o pedido. Se o pedido for aumentar o limite e o Score n√£o for forte o suficiente, a IA dinamicamente engatilha uma entrevista contextual para extrair os dados financeiros brutos do indiv√≠duo (renda, dividas, dependentes), re-avaliar o grau de risco (Score) num arquivo em banco de dados e aplicar o aumento no limite.

---

## üèó Arquitetura do Sistema e Fluxos

O Back-end foi constru√≠do em arquitetura modular utilizando **Python e FastAPI**. O ecossistema √© quebrado da seguinte forma l√≥gica:

### ‚úÖ 1. Roteamento e Agentes Independentes (`api/routers/`)
Para evitar c√≥digo "Spaghetti", o core de neg√≥cio foi segmentado em 4 Agentes Aut√¥nomos (Micro-Bots):

- **Agente de Triagem (`triagem.py`)**: Recepcionista do banco. Autentica via CPF e Data de Nascimento (`clientes.csv`). Capta a primeira mensagem de necessidade do cliente ("O que deseja fazer?") e realiza o Parsing usando LangChain para transferi-lo de forma impl√≠cita e sorrateira para o Agente especialista respons√°vel.
- **Agente de Cr√©dito (`credito.py`)**: Gerencia consultas de saldo e requisi√ß√µes de aumento. Consulta e consolida regras de neg√≥cio puras (ex: "Score atual de 300 permite no m√°ximo R$2.000 de limite?"). Se necess√°rio, transfere o contexto para o agente de entrevista.
- **Agente de Entrevista (`entrevista.py`)**: Atuando como Perito em Risco. Faz uma entrevista humanizada perguntando de ocupa√ß√£o profissional at√© volume de dependentes. Utiliza **Extrativismo Estruturado de Dados** com o LangChain (`JsonOutputParser`) para capturar respostas vagas textuais e converter em JSON estrito. Executa a f√≥rmula de rec√°lculo de Score e grava num pseudo-banco de dados est√°tico. Transfere os resultados de volta ao cr√©dito.
- **Agente de C√¢mbio (`cambio.py`)**: Consome a API REST externa (AwesomeAPI) sob demanda. Puxa do LLM a entidade "Nome de Moeda", converte para as 3 Letras ISO Globais (ex: 'Libra' -> 'GBP') e devolve uma cota√ß√£o instant√¢nea versus BRL.

### ‚úÖ 2. Servi√ßos Globais 
- **LLM Service (`api/llm_service.py`)**: Central controladora do LangChain que consome o **Ollama (Llama 3.2)** para todos os subagentes. Suporta Prompt Engineering centralizado e Try/Catch preventivos de *Fallback* para quando a rede/IA falhar.
- **Session Service (`api/sessao.py`)**: Atua como uma mem√≥ria RAM/Cache "In-Memory" para intera√ß√µes do front. Armazena o ID da aba, hist√≥rico de chat (`Role: Content`) para fornecer Context Window √† LLM e Vari√°veis de Estado (M√°quina de Estados de conversa√ß√£o - `AGUARDANDO_VALOR`, `MENU`, `AUTENTICADO`).

---

## üöÄ Funcionalidades Implementadas

- **Autentica√ß√£o em Dois Passos**: Exige Valida√ß√£o sequencial do CPF (apenas 11 d√≠gitos, ignorando pontua√ß√£o na interface) seguido de Data de Nascimento.
- **Cota√ß√£o de Moedas em Tempo Real**: Uso de API P√∫blica gratuita ("AwesomeAPI") baseada na identifica√ß√£o da inten√ß√£o monet√°ria ("Quanto est√° o BTC?").
- **Workflow de Rec√°lculo Concedido de Limite**: Abordagem inteligente onde um score ruim n√£o encerra a jornada precocemente, mas permite segunda chance via an√°lise de viabilidade atual (Entrevista).
- **Single-Agent Illusion**: Bot√µes baseados em 'Outros Servi√ßos'. Os roteamentos t√©cnicos (`acao: transferir, alvo: AgenteCredito`) n√£o s√£o revelados ao usu√°rio, parecendo uma conversa fluida num √∫nico c√©rebro virtual inteligente.
- **Resili√™ncia de Hardware e Quedas (Try/Catch)**: Sistema tolerante a desastres, capturando exce√ß√µes geradas por arquivos corrompidos baseados em travas (CSVs abertos) e travamentos bruscos do Backend Local AI (Ollama demorando para responder gera tratativa customizada ao inv√©s de tela congelada).
- **Interface Front-end (UI/UX)**: Layout desacoplado com Dark Mode minimalista "Tech for Humans". Utiliza√ß√£o massiva de Quick-Replies interativos e bal√£o emulativo de digita√ß√£o ass√≠ncrono (Typing Indicator).

---

## ‚öî Desafios Enfrentados e Solu√ß√µes

1. **Race Condition na Classifica√ß√£o de Inten√ß√µes (Alucina√ß√£o)**:
   - *Desafio*: O Llama (Open-Source e leve) frequentemente gosta de dar explica√ß√µes. Na hora da Triagem para pedir para cotar moeda, ele dizia *"Sua inten√ß√£o n√£o √© Entrevista. Vejo que quer uma cota√ß√£o de moedas (C√¢mbio)"*. O sistema Python, por ter lido primeiro a palavra *"Entrevista"*, errava em 10% dos casos a rota.
   - *Solu√ß√£o*: Alterada a heur√≠stica de varredura Python (Apenas "if palavra in inten√ß√£o"). Foi constru√≠da uma matriz Array iter√°vel que para no momento (`break`) que acha o primeiro Match correspondente expl√≠cito sem pegar os ruidos residuais do Output da LLM.

2. **Extra√ß√£o de Dados Imprecisos durante a Entrevista**:
   - *Desafio*: O prompt extra√≠a "Possui dividas?" como "sim, tenho" ou "√±". Isso quebrava o parser Json e n√£o alimentava os c√°lculos do novo de Score.
   - *Solu√ß√£o*: Implementado o `JsonOutputParser` nativo do framework **LangChain** garantindo output JSON Strict, formatando a temperatura em `0.0` e usando chaves "Null", unindo a Fallbacks do Python ("Se n√£o vier emprego Formal, limpa e for√ßa na ra√ßa Aut√¥nomo ou None...").

3. **Demora excessiva para responder no front-end**:
   - *Desafio*: Para cada letra ou bot√£o clicado, o bot visual parecia "travado" ou recarregava inteiramente o index.html antes do rob√¥ trazer uma resposta, dando sensa√ß√£o de que o servidor era quebrado.
   - *Solu√ß√£o*: Refatora√ß√£o da mec√¢nica do DOM com chamadas JavaScript puras `Fetch API` para n√£o dar refresh na p√°gina e inje√ß√£o do div `Typing Indicator` enquanto se aguarda o sinal do status-code 200 do FastAPI.

---

## üõ† Escolhas T√©cnicas e Justificativas

- **FastAPI**: Escolhido pelo baixo overhead e pelo assincronismo (`async def`) nativo, o que √© mandat√≥rio ao aguardar um processo extremamente lento como chamadas de LLM ou chamadas HTTP para Cota√ß√£o. Tamb√©m possui auto-documenta√ß√£o e roteador simples (`APIRouter`) ideal pra nossa separa√ß√£o em Agentes.
- **Vanilla JS + HTML + CSS**: Foi preterido o uso de ReactJS ou Vue. Como sendo um desafio pr√°tico focado na IA, uma aplica√ß√£o robusta frontend puro garante aus√™ncia de depend√™ncias NPM inchadas (`node_modules`), sendo s√≥ dar F5 e ver funcionando sem atritos de instala√ß√£o Webpack/Vite para os avaliadores.
- **Ollama / Llama-3.2**: A escolha de uma Stack `Local-First` foi intencional para provar profici√™ncia de ponta a ponta que independe da conta corporativa das OpenAI. O modelo "Llama 3.2" √© perform√°tico, pode rodar at√© em modesto hardware caseiro e exibe racioc√≠nio de alto n√≠vel.
- **LangChain**: Excelente em lidar com "Output Resolvers" e encadear mensagens contextuais em templates vari√°veis (Pipeline Chain -> `prompt | llm | string_parser`).
- **Arquivos CSV como DB**: Op√ß√£o amig√°vel e puramente simulativa para n√£o impor que o time t√©cnico Tech for Humans necessite ligar containers Docker PostgreSQL na avalia√ß√£o.

---

## üïπ Tutorial de Execu√ß√£o e Testes

### Pr√©-requisitos
Ter instalado no sistema operacional:
- **Python 3.10+**
- **Ollama** (para rodar a IA localmente sem chave da OpenAI). Baixe em: `https://ollama.com`

### 1. Preparando o Ambiente (Backend)
1. Clone este reposit√≥rio para seu ambiente local.
2. Na raiz da pasta, crie um ambiente virtual (Opcional, mas recomendado):
   ```bash
   python -m venv venv
   # Ative-o:
   # No Windows: venv\Scripts\activate
   # No Mac/Linux: source venv/bin/activate
   ```
3. Instale as depend√™ncias:
   ```bash
   pip install -r requirements.txt
   ```

### 2. Baixando o Modelo de Intelig√™ncia Artificial Local
Inicie o processo do seu "Motor" Ollama no terminal para puxar o c√©rebro usado em nosso Agente:
```bash
ollama run llama3.2
```
*Se for a primeira vez, ele ir√° baixar o modelo (cerca de 2GB).* Assim que iniciar um prompt shell `>>>`, pode fech√°-lo (Ctrl+D), o modelo j√° estar√° armazenado e seu hardware habilitado para ouvi-lo.

### 3. Rodando a Aplica√ß√£o
Dentro da pasta `api/` execute o servidor ASGI Uvicorn:
```bash
cd api
python main.py
```
O console dever√° mostrar o servidor rodando em `http://0.0.0.0:8000`.

### 4. Abrindo o Chat e Testando
Nenhum compilador adicional front-end √© requerido!
Basta encontrar o arquivo `index.html` na pasta `/frontend` e abri-lo clicando duas vezes no seu navegador de prefer√™ncia (Chrome, Edge, etc).

* **Teste 1 (Autentica√ß√£o)**: Envie o CPF de John Doe simulado no CSV `12345678901`, informando `01/01/1990` na Data.
* **Teste 2 (C√¢mbio)**: Clique em op√ß√µes/digite "Gostaria de cotar o USD". 
* **Teste 3 (Roteamento de Entrevista Elevada)**: Com John Doe, pe√ßa para *Solicitar Aumento*, Pe√ßa a quantidade esbanjadora de `6500` reais. O limite ser√° rejeitado por score baixo. Aceite a entrevista, d√™ informa√ß√µes como *Ocupa√ß√£o Formal*, *Renda alta*, e *Nenhuma d√≠vida*. Observe o Score subir em tempo real e em seguida refa√ßa a requisi√ß√£o do cr√©dito e colha os frutos do aprova√ß√£o.